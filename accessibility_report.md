# Сравнительный анализ инструментов автоматизированной проверки доступности

Автор: Исследовательская группа

Дата: 21 января 2026

## Аннотация

В работе представлен сравнительный анализ трёх инструментов автоматизированной проверки доступности: `axe-core`, `QualWeb` и `Alfa`. Цель исследования — оценить точность (precision), полноту (recall), сбалансированную F1-меру и производительность (время выполнения) на корпусе тест-кейсов. Результаты получены на основе бенчмарков, запущенных в единой среде исполнения. Для каждого инструмента вычислены метрики классификации (TP/FP/FN/TN), производственные показатели и интерпретация значений.

## Введение

Автоматизированные инструменты оценки доступности широко используются в промышленности и научных исследованиях для первичной валидации соответствия веб-контента стандартам WCAG и ACT. Однако инструменты отличаются по подходам к детекции, покрытию правил и числу ложных срабатываний. В рамках НИР важно не только сравнить «сырые» обнаружения, но и оценить практическую применимость — сколько ручной проверки требуется и какие классы ошибок чаще пропускаются.

## Объект и предмет исследования

- Объект: автоматизированные инструменты проверки доступности.
- Предмет: сравнительная оценка точности, полноты и производительности `axe-core`, `QualWeb` и `Alfa` на стандартном корпусе тест-кейсов.

## Обзор инструментов и мотивация выбора

- `axe-core`: промышленный движок с широкой экосистемой, ориентирован на практическое применение, активно используется в CI/CD.
- `QualWeb`: исследовательская платформа/агрегатор, акцент на воспроизводимости и расширяемости для научных исследований.
- `Alfa` (siteimprove/alfa): правило-ориентированный движок, направленный на формальную проверку соответствия ACT и верифицируемость правил.

Причина выбора: эти три инструмента представляют разные методологические подходы (инженерный, исследовательский и формализованный), открыты и пригодны для пакетного запуска, что обеспечивает воспроизводимость и исследовательскую репликацию.

## Материалы и исходные данные

Результаты бенчмарков находятся в папке `results/` репозитория. В работе использованы файлы:

- `results/axe_results.txt` — выход `axe-core`.
- `results/qualweb_results.txt` — выход `QualWeb`.
- `results/alfa_results.txt` — выход `Alfa`.

Скрипты для запуска: `benchmark-axe.js`, `benchmark-qualweb.js`, `benchmark-alfa.js`.

## Методика исследования

- Корпус: набор тест-кейсов страниц (размеры корпусов приведены в разделе «Результаты»).
- Конфигурация: дефолтные параметры каждого инструмента без пользовательских фильтров; среда — локальная машина Linux с Node.js; последовательный запуск бенчмарков.
- Процесс: для каждой страницы сравнение вывода инструмента с эталонной разметкой (ground truth). Для каждой проверки собираются TP, FP, FN, TN и время выполнения.
- Репликация: рекомендовано проводить ≥3 запуска для оценки вариативности; в текущей работе использован один прогон для каждого инструмента (см. ограничения).
- Ограничения: возможны различия в наборе обработанных тест-кейсов между инструментами; `Alfa` был запущен отдельно и может иметь другие настройки по умолчанию. В дальнейшем следует синхронизировать корпус и повторить запуски.

## Метрики и обоснование выбора

Применены стандартные метрики бинарной классификации и показатели производительности:

- Precision (точность): $\mathrm{Precision} = \frac{TP}{TP + FP}$. Отражает долю найденных проблем, которые являются истинными проблемами; важна для оценки объёма ручной верификации.
- Recall (полнота): $\mathrm{Recall} = \frac{TP}{TP + FN}$. Отражает долю реальных проблем, найденных автоматом; важна для оценки покрытия.
- F1-Score: $\mathrm{F1} = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Сбалансированная метрика при компромиссе между Precision и Recall.
- Accuracy: $\mathrm{Accuracy} = \frac{TP + TN}{Total}$. Общая доля правильно классифицированных случаев.
- Specificity: $\mathrm{Specificity} = \frac{TN}{TN + FP}$. Отражает способность не ошибочно отмечать чистые элементы.
- FPR / FNR: показатели ложных срабатываний и пропусков.
- Производительность: среднее время на URL (`Average Time per URL`), суммарное время выполнения (`Total Execution Time`), пропускная способность (Throughput = Total / TotalExecutionTime).

Выбор этих метрик обусловлен необходимостью оценить одновременно надёжность выводов, полноту обнаружения и практическую применимость при массовой проверке.

## Результаты

Ниже приведены исходные агрегированные результаты, извлечённые из файлов бенчмарков.

### Alfa

```
Total test cases processed: 684
True Positives (TP): 175
False Positives (FP): 8
False Negatives (FN): 39
True Negatives (TN): 255
Precision: 0.9563
Recall: 0.8178
F1-Score: 0.8816
Average Time per URL: 1.1025s
Total Execution Time: 189.05s
```

Вычисленные дополнительные метрики:

- Total = 684
- Accuracy = (175 + 255) / 684 = 0.6265
- Specificity = 255 / (255 + 8) = 0.9696
- FPR = 8 / (8 + 255) = 0.0304
- FNR = 39 / (39 + 175) = 0.1822
- Throughput ≈ 684 / 189.05 ≈ 3.62 URLs/s

### axe-core

```
Total test cases processed: 714
True Positives (TP): 167
False Positives (FP): 3
False Negatives (FN): 57
True Negatives (TN): 282
Precision: 0.9824
Recall: 0.7455
F1-Score: 0.8477
Average Time per URL: 1.3905s
Total Execution Time: 154.21s
```

Вычисленные дополнительные метрики:

- Total = 714
- Accuracy = (167 + 282) / 714 = 0.6286
- Specificity = 282 / (282 + 3) = 0.9895
- FPR = 3 / (3 + 282) = 0.0105
- FNR = 57 / (57 + 167) = 0.2545
- Throughput ≈ 714 / 154.21 ≈ 4.63 URLs/s

Примечание: наблюдается несоответствие между `Average Time per URL` и `Total Execution Time` (1.3905 * 714 ≈ 992s ≠ 154.21s). Требуется проверка логики агрегирования времени в скрипте `benchmark-axe.js`.

### QualWeb

```
Total test cases processed: 856
True Positives (TP): 185
False Positives (FP): 1
False Negatives (FN): 100
True Negatives (TN): 332
Precision: 0.9946
Recall: 0.6491
F1-Score: 0.7856
Average Time per URL: 0.6263s
Total Execution Time: 536.21s
```

Вычисленные дополнительные метрики:

- Total = 856
- Accuracy = (185 + 332) / 856 = 0.6030
- Specificity = 332 / (332 + 1) = 0.9970
- FPR = 1 / (1 + 332) = 0.0030
- FNR = 100 / (100 + 185) = 0.3509
- Throughput ≈ 856 / 536.21 ≈ 1.60 URLs/s

## Анализ результатов

- Precision: все три инструмента демонстрируют высокую точность (>0.95), при этом `QualWeb` и `axe-core` достигают наивысших значений (≈0.995 и 0.982 соответственно). Это указывает на то, что найденные дефекты редко оказываются ложными тревогами.
- Recall: наибольшая полнота у `Alfa` (0.8178), затем `axe-core` (0.7455), и `QualWeb` (0.6491). Значит, `Alfa` в данном корпусе находит больше реальных проблем.
- F1-Score: `Alfa` (0.8816) > `axe-core` (0.8477) > `QualWeb` (0.7856) — по сбалансированной метрике `Alfa` показывает лучшее соотношение точности и полноты.
- Specificity и FPR: `QualWeb` и `axe-core` показывают очень низкие FPR (<1%), что минимизирует объём ручной проверке найденных ошибок.
- Производительность: наблюдаются различия в `Average Time per URL` и в `Total Execution Time`; требуется унифицировать сбор временных метрик. При текущих данных `axe-core` демонстрирует высокую пропускную способность (в пересчёте по заявленному Total Execution Time), но этот показатель может быть некорректен из-за расхождений в агрегировании времени.
- Ограничение: все инструменты имеют ненулевой FNR — автотесты пропускают от ~18% (`Alfa`) до ~35% (`QualWeb`) реальных дефектов в данном корпусе. Следует учитывать необходимость ручной проверки или комбинирования инструментов.

## Обсуждение

1. Для практических задач, где требуется минимизировать ручную валидацию найденных дефектов, предпочтителен `QualWeb` или `axe-core` из-за высокой Precision и низкого FPR.
2. При приоритете максимального покрытия ошибок лучше использовать `Alfa` или комбинировать `Alfa` и `axe-core` — это повысит Recall при приемлемом росте числа ложных тревог.
3. Для научных исследований важно проводить многократные прогоны и оценивать статистическую значимость различий (например, тесты на разницу долей для Recall/Precision или бутстрап-оценки для F1).
4. Производственные метрики времени должны собираться унифицированно: рекомендую править скрипты `benchmark-*.js` так, чтобы `Total Execution Time` — это суммарное реальное время ветки процесса, а `Average Time per URL` — среднее арифметическое по обработанным URL.

## Выводы

- В рассмотренном корпусе все три инструмента демонстрируют высокую точность, однако различаются по полноте и поведению по времени.
- `Alfa` показывает лучший F1 и высокую Recall, что делает его ценным в задачах верификации правил и научных анализах покрытия.
- `axe-core` обеспечивает хороший баланс между полнотой и точностью, является практичным выбором для CI/CD.
- `QualWeb` минимизирует ложные срабатывания и быстрее обрабатывает URL по среднему времени, но пропускает больше реальных дефектов.
- Рекомендую комбинировать инструменты и провести дополнительные прогоны (≥3) с синхронизированным корпусом и исправленной агрегацией временных метрик для окончательных заключений и публикации результатов.

## Рекомендации по дальнейшим шагам

1. Исправить сбор временных метрик в `benchmark-axe.js` и повторить замеры.
2. Синхронизировать корпус тест-кейсов для всех инструментов (равное `Total`).
3. Провести ≥3 повторных прогона и оценить доверительные интервалы метрик.
4. Добавить анализ по категориям ошибок (тип правила WCAG/ACT), чтобы понять, какие классы проблем пропускаются чаще.
5. Подготовить материалы для публикации: таблицы TP/FP/FN/TN, графики Precision–Recall, и репозиторий со скриптами для воспроизводимости.

## Приложения

- Скрипты запуска: `benchmark-axe.js`, `benchmark-qualweb.js`, `benchmark-alfa.js`.
- Маппинги правил: `alfa-to-act-mapping.json`, `axe-to-act-mapping.json`.
- Результаты: `results/axe_results.txt`, `results/qualweb_results.txt`, `results/alfa_results.txt`.

---

Файл с отчётом создан: `accessibility_report.md`.
